**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group \#21:      |     |
| -------------- | --- |
| Student Names: | Tony Tan |
|                | Evan Wong |
|                | Johnny Yuen |
|                | Bryant Zhang |

(Note that some labs require individual reports while others require one report
for each group. Please see each lab document for details.)

# 1 Introduction

The purpose of this lab is to take our experiences learned from the previous lab (lab 2: requirements based test generation) and continue it, expanding our knowledge through unit testing by using JUnit in Eclipse. This lab will expand the test suites to improve branch, statements, and method coverage. The lab will be done through white box testing methods.

# 2 Manual data-flow coverage calculations for X and Y methods

Data Flow Graph for Range test getLength():
![image](https://user-images.githubusercontent.com/101444825/222853295-77518956-a93a-42f2-bba5-eb873943f480.png)

Def-use sets per statement:
Lower:
du(1,x) =
	{[1,4]
	 [1,2]}

Upper:
du(1,x) =
	{[1,4]
	 [1,2]}

msg:
du(2,3) = 
{[2,3]}

DU-pairs per variable:
Variable		Pairs
Lower:  		(1,4), (1,2)
Upper: 		(1,4), (1,2)
Msg:    		(2,3)

DU-Pair coverage:
Test								Pairs Covered		Coverage PercentagelengthTestWithDifferentRangesPositive()	(1,4) (1,4)		40% or ⅖ 


![DataUtilities calculateColumnTotal Data Flow Graph drawio](https://user-images.githubusercontent.com/101444825/222382226-6384041f-7400-4609-9533-61730b122f16.png)

Defs
def(1) = {data, column} 
def(3) = {total} 
def(4) = {rowCount} 
def(5) = {r} 
def(6) = {n} 
def(11) = {r2} 
def(12) = {n} 

Uses
use(2) = {data}
use(4) = {data}
use(5) = {r, rowCount}
use(6) = {data, r, column}
use(7) = {n}
use(8) = {total, n}
use(9) = {r}
use(11) = {r2, rowCount}
use(12) = {data, r2, rowCount}
use(13) = {n}
use(14) = {total, n}
use(15) = {r2}
use(16) = {total}

DU-pairs
data: (1, 2), (1, 4), (1, 6), (1, 12)
column: (1, 6), (1, 12)
total: (3, 8), (3, 14), (3, 16)
rowCount: (4, 5), (4, 6), (4, 11), (4, 12)
r: (5, 6), (5, 9)
n: (6, 7), (6, 8), (12, 13), (12, 14)
r2: (11, 12), (11, 15)

Pairs Covered
calculateColumnTotalForTwoValues(): (1, 2), (1, 4), (1,6), (3, 8), (3, 16), (4, 5), (4, 6), (4, 11), (5, 6), (5, 9), (6, 7), (6, 8)

calculateColumnTotalForNull(): (1, 2)

calculateColumnTotalForInvalidColumn(): (1, 2), (1, 4), (1,6), (3, 8), (3, 16), (4, 5), (4, 6), (4, 11), (5, 6), (5, 9), (6, 7), (6, 8)

calculateColumnTotalForTable(): (1, 2), (1, 4), (1,6), (3, 8), (3, 16), (4, 5), (4, 6), (4, 11), (5, 6), (5, 9), (6, 7), (6, 8)

DU-Pair Coverage:
Covered DU-pairs / Total DU-pairs
(12 / 21) * 100% = 57.14%


# 3 A detailed description of the testing strategy for the new unit test

The testing strategy we will be using for the new unit tests is to split Range and DataUtilities into two groups of two. In these two groups of two we will develop tests that cover each line of code within their respective classes. We will use coverage-based test generation in order to successfully fulfill the coverage percentages needed. We will keep our tests as specific as possible in order to keep consistency and readability within our tests. After tests are successfully coded, groups will switch and review each other's tests looking for inconsistencies or detects. Afterwards both groups will join up for one final inspection and overview of the tests and code. This will ensure that the code coverage is up to the requirements and adequate peer-programming has been done.

Initial Test Case Coverage:

DataUtilities.java
Branch coverage: 32.8%
![image](https://user-images.githubusercontent.com/101444825/222854980-d39179e0-c1ca-45ea-ac3a-7459ac8b7c33.png)

Line coverage: 45.8%
![image](https://user-images.githubusercontent.com/101444825/222855010-354764b2-346d-4f2b-b16d-3ca812a0b8bf.png)

Method coverage: 50.0%
![image](https://user-images.githubusercontent.com/101444825/222855044-9bc31991-3ea1-464e-b878-37871c277083.png)


Range.java
Branch coverage: 18.3%
![image](https://user-images.githubusercontent.com/101444825/222854714-894e7d43-564c-4250-a5ca-570c9a83831a.png)

Line coverage: 20.2%
![image](https://user-images.githubusercontent.com/101444825/222854868-17f6c74d-3f22-47d9-8db2-327471a41331.png)

Method coverage: 26.1%
![image](https://user-images.githubusercontent.com/101444825/222854907-d43d8910-82d9-4315-80d1-e291408ba4ec.png)

# 4 A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage

Text…

# 5 A detailed report of the coverage achieved of each class and method (a screen shot from the code cover results in green and red color would suffice)

DataUtilities

Branch Coverage: 32.8% → 75.0%
![image](https://user-images.githubusercontent.com/101444825/222853562-fd93e3b0-6228-4bfd-8408-3d494f24d867.png)


Line Coverage: 45.8% → 90.6%
![image](https://user-images.githubusercontent.com/101444825/222853585-277d8779-fa74-463d-9df2-2ed8bfecde7f.png)


Method Coverage: 50.0% → 90.0%
![image](https://user-images.githubusercontent.com/101444825/222853617-74fc52c6-ba4a-49d3-b8b9-fff377a1bde1.png)


Range
Branch coverage: 18.3% -> 81.7%
![image](https://user-images.githubusercontent.com/101444825/222853650-499637be-0810-4d11-9c39-3d10ac5b6dff.png)


Line coverage: 20.2% -> 89.1%
![image](https://user-images.githubusercontent.com/101444825/222853673-76e3865e-7125-4c30-8f37-b0dde439c4ce.png)


Method coverage: 26.1% -> 100%
![image](https://user-images.githubusercontent.com/101444825/222853687-f17247d7-fb3e-486b-9802-93e50b662ef6.png)



# 6 Pros and Cons of coverage tools used and Metrics you report

The coverage tool we used in this lab is EclEmma because it was recommended to be used for this lab and it was already pre installed with Eclipse. We were also familiar with using Eclipse since we completed the previous lab with Eclipse. Although EclEmma did not explicitly have all of the suggested coverage metrics to report, we were able to replace statement and condition coverages with line coverage and method coverage. 
One of the cons with using line coverage is that it is affected by code format, comments and white space in the code which may not be as accurate as statement coverage. One of the cons with using method coverage instead of condition coverage is that method coverage only measures the methods that have been executed during tests whereas condition coverage measures all of the potential condition outcomes that are possible which provides more information.


# 7 A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.

The advantages of requirements based test generation is that it makes sure that the test cases being made meet all the requirements requested. Requirements based testing helps detect faults in software in the early stages and makes sure all requirements are covered. It also makes sure that the critical functionalities of the system are covered. The disadvantage is that it doesn’t test all the possible scenarios making it so that some defects go undetected in the software causing problems in the future. There also comes the problem of the priority of testing requirements, sometimes it is a challenge to rank the requirements leading to inefficient testing.

The advantages of coverage based testing is that it makes sure that all of the code is being tested, every single line executed at least once. Coverage based testing will test all possible scenarios in a software application. This approach helps detect defects that wouldn’t have gotten tested in requirements-based testing. Another advantage would be a good measurement of how well tested the code is, as well as identifying where code is untested, or poorly testing. The disadvantage of this is that it is extremely time consuming for larger programs as well as the possibility of having some requirements go untested. It will also lead to testing of code that isn’t at all relevant to the overall software, and even with high coverage, it doesn’t mean that there will be no defects.

The comparison between the two is that requirements-based testing makes sure that the software application will meet all the specified requirements, while coverage based testing will achieve comprehensive testing of code. So overall it is based on the goals of the developer on which testing is necessary for their project.


# 8 A discussion on how the team work/effort was divided and managed

How the team’s work and effort was divided and managed was at first we split into teams, and allocated tasks evenly and fairly between the two groups with a timeline of tasks. By using peer programming we increased efficiency and decreased mistakes when programming our test code. After everything was done, we combined the work together and tested and ensured as a group that everything was working correctly. 

# 9 Any difficulties encountered, challenges overcome, and lessons learned from performing the lab

Difficulties encountered were when setting up the lab images did not show up, as well as familiarizing ourselves with EclEmma. Had slight challenges when creating new tests, and improving old tests for coverage, which were eventually overcome. Lessons learnt were good white-box testing methods, how to provide good coverage for code, drawing data flow graphs and using and calculating def-use cases and pairs.

# 10 Comments/feedback on the lab itself

Lab was difficult to set up. When following the lab instructions, some steps were very unclear, and images to verify steps were giving errors (404 page not found). This made setting up very difficult, finding missing reference libraries. Videos to help guide students would help resolve this issue. 

